ialized BlockManager: BlockManagerId(driver, 192.168.101.140, 43029, None)
19/01/25 20:58:59 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@696bf46e{/metrics/json,null,AVAILABLE,@Spark}
19/01/25 20:59:02 INFO scheduler.EventLoggingListener: Logging events to hdfs://hadoopMaster:9000/spark-logs/local-1548449939150
Reading directory: '1 de enero de 2014'
19/01/25 20:59:03 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 280.9 KB, free 3.0 GB)
19/01/25 20:59:03 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 3.0 GB)
19/01/25 20:59:03 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.101.140:43029 (size: 23.3 KB, free: 3.0 GB)
19/01/25 20:59:03 INFO spark.SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
Start data filter
End data filter
Start computing Term Frequency Vectors
End computing Term Frequency Vectors
Start computing IDF
19/01/25 20:59:05 INFO input.FileInputFormat: Total input paths to process : 102
19/01/25 20:59:06 INFO input.FileInputFormat: Total input paths to process : 102
19/01/25 20:59:06 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 4, size left: 22988
19/01/25 20:59:07 INFO spark.SparkContext: Starting job: treeAggregate at IDF.scala:54
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Got job 0 (treeAggregate at IDF.scala:54) with 2 output partitions
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (treeAggregate at IDF.scala:54)
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Missing parents: List()
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at IDF.scala:54), which has no missing parents
19/01/25 20:59:07 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 3.0 GB)
19/01/25 20:59:07 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 3.0 GB)
19/01/25 20:59:07 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.101.140:43029 (size: 4.6 KB, free: 3.0 GB)
19/01/25 20:59:07 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
19/01/25 20:59:07 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at treeAggregate at IDF.scala:54) (first 15 tasks are for partitions Vector(0, 1))
19/01/25 20:59:07 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
19/01/25 20:59:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, localhost, executor driver, partition 1, PROCESS_LOCAL, 13566 bytes)
19/01/25 20:59:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, localhost, executor driver, partition 0, ANY, 5518 bytes)
19/01/25 20:59:07 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 1)
19/01/25 20:59:07 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 0)
19/01/25 20:59:07 INFO executor.Executor: Fetching file:/home/hadoop/wb-test-01/tfidf.py with timestamp 1548449938964
19/01/25 20:59:08 INFO util.Utils: /home/hadoop/wb-test-01/tfidf.py has been previously copied to /tmp/spark-7ad65664-0d14-4839-aacc-0136eaecf10c/userFiles-5155cffa-b2d6-42c0-90f2-333c5f41f687/tfidf.py
19/01/25 20:59:08 INFO rdd.WholeTextFileRDD: Input split: Paths:/user/hadoop/cooperativa/20140101085418/part-00005:0+2113,/user/hadoop/cooperativa/20140101090434/part-00005:0+3307,/user/hadoop/cooperativa/20140101091138/part-00005:0+1730,/user/hadoop/cooperativa/20140101100443/part-00005:0+7201,/user/hadoop/cooperativa/20140101104619/part-00005:0+3179,/user/hadoop/cooperativa/20140101110939/part-00005:0+3196,/user/hadoop/cooperativa/20140101111112/part-00005:0+2326
19/01/25 20:59:08 INFO rdd.WholeTextFileRDD: Input split: Paths:/user/hadoop/cooperativa/20140101085418/part-00000:0+0,/user/hadoop/cooperativa/20140101085418/part-00001:0+0,/user/hadoop/cooperativa/20140101085418/part-00002:0+0,/user/hadoop/cooperativa/20140101085418/part-00003:0+0,/user/hadoop/cooperativa/20140101085418/part-00004:0+0,/user/hadoop/cooperativa/20140101090434/part-00000:0+0,/user/hadoop/cooperativa/20140101090434/part-00001:0+0,/user/hadoop/cooperativa/20140101090434/part-00002:0+0,/user/hadoop/cooperativa/20140101090434/part-00003:0+0,/user/hadoop/cooperativa/20140101090434/part-00004:0+0,/user/hadoop/cooperativa/20140101091138/part-00000:0+0,/user/hadoop/cooperativa/20140101091138/part-00001:0+0,/user/hadoop/cooperativa/20140101091138/part-00002:0+0,/user/hadoop/cooperativa/20140101091138/part-00003:0+0,/user/hadoop/cooperativa/20140101091138/part-00004:0+0,/user/hadoop/cooperativa/20140101100443/part-00000:0+0,/user/hadoop/cooperativa/20140101100443/part-00001:0+0,/user/hadoop/cooperativa/20140101100443/part-00002:0+0,/user/hadoop/cooperativa/20140101100443/part-00003:0+0,/user/hadoop/cooperativa/20140101100443/part-00004:0+0,/user/hadoop/cooperativa/20140101104619/part-00000:0+0,/user/hadoop/cooperativa/20140101104619/part-00001:0+0,/user/hadoop/cooperativa/20140101104619/part-00002:0+0,/user/hadoop/cooperativa/20140101104619/part-00003:0+0,/user/hadoop/cooperativa/20140101104619/part-00004:0+0,/user/hadoop/cooperativa/20140101110939/part-00000:0+0,/user/hadoop/cooperativa/20140101110939/part-00001:0+0,/user/hadoop/cooperativa/20140101110939/part-00002:0+0,/user/hadoop/cooperativa/20140101110939/part-00003:0+0,/user/hadoop/cooperativa/20140101110939/part-00004:0+0,/user/hadoop/cooperativa/20140101111112/part-00000:0+0,/user/hadoop/cooperativa/20140101111112/part-00001:0+0,/user/hadoop/cooperativa/20140101111112/part-00002:0+0,/user/hadoop/cooperativa/20140101111112/part-00003:0+0,/user/hadoop/cooperativa/20140101111112/part-00004:0+0,/user/hadoop/cooperativa/20140101121239/part-00000:0+0,/user/hadoop/cooperativa/20140101121239/part-00001:0+0,/user/hadoop/cooperativa/20140101121239/part-00002:0+0,/user/hadoop/cooperativa/20140101121239/part-00003:0+0,/user/hadoop/cooperativa/20140101121239/part-00004:0+0,/user/hadoop/cooperativa/20140101121239/part-00005:0+2166,/user/hadoop/cooperativa/20140101130956/part-00000:0+0,/user/hadoop/cooperativa/20140101130956/part-00001:0+0,/user/hadoop/cooperativa/20140101130956/part-00002:0+0,/user/hadoop/cooperativa/20140101130956/part-00003:0+0,/user/hadoop/cooperativa/20140101130956/part-00004:0+0,/user/hadoop/cooperativa/20140101130956/part-00005:0+2058,/user/hadoop/cooperativa/20140101134236/part-00000:0+0,/user/hadoop/cooperativa/20140101134236/part-00001:0+0,/user/hadoop/cooperativa/20140101134236/part-00002:0+0,/user/hadoop/cooperativa/20140101134236/part-00003:0+0,/user/hadoop/cooperativa/20140101134236/part-00004:0+0,/user/hadoop/cooperativa/20140101134236/part-00005:0+2618,/user/hadoop/cooperativa/20140101134348/part-00000:0+0,/user/hadoop/cooperativa/20140101134348/part-00001:0+0,/user/hadoop/cooperativa/20140101134348/part-00002:0+0,/user/hadoop/cooperativa/20140101134348/part-00003:0+0,/user/hadoop/cooperativa/20140101134348/part-00004:0+0,/user/hadoop/cooperativa/20140101134348/part-00005:0+2426,/user/hadoop/cooperativa/20140101140355/part-00000:0+0,/user/hadoop/cooperativa/20140101140355/part-00001:0+0,/user/hadoop/cooperativa/20140101140355/part-00002:0+0,/user/hadoop/cooperativa/20140101140355/part-00003:0+0,/user/hadoop/cooperativa/20140101140355/part-00004:0+0,/user/hadoop/cooperativa/20140101140355/part-00005:0+1802,/user/hadoop/cooperativa/20140101141300/part-00000:0+0,/user/hadoop/cooperativa/20140101141300/part-00001:0+0,/user/hadoop/cooperativa/20140101141300/part-00002:0+0,/user/hadoop/cooperativa/20140101141300/part-00003:0+0,/user/hadoop/cooperativa/20140101141300/part-00004:0+0,/user/hadoop/cooperativa/20140101141300/part-00005:0+2806,/user/hadoop/cooperativa/20140101150649/part-00000:0+0,/user/hadoop/cooperativa/20140101150649/part-00001:0+0,/user/hadoop/cooperativa/20140101150649/part-00002:0+0,/user/hadoop/cooperativa/20140101150649/part-00003:0+0,/user/hadoop/cooperativa/20140101150649/part-00004:0+0,/user/hadoop/cooperativa/20140101150649/part-00005:0+2323,/user/hadoop/cooperativa/20140101164734/part-00000:0+0,/user/hadoop/cooperativa/20140101164734/part-00001:0+0,/user/hadoop/cooperativa/20140101164734/part-00002:0+0,/user/hadoop/cooperativa/20140101164734/part-00003:0+0,/user/hadoop/cooperativa/20140101164734/part-00004:0+0,/user/hadoop/cooperativa/20140101164734/part-00005:0+1435,/user/hadoop/cooperativa/20140101194322/part-00000:0+0,/user/hadoop/cooperativa/20140101194322/part-00001:0+0,/user/hadoop/cooperativa/20140101194322/part-00002:0+0,/user/hadoop/cooperativa/20140101194322/part-00003:0+0,/user/hadoop/cooperativa/20140101194322/part-00004:0+0,/user/hadoop/cooperativa/20140101194322/part-00005:0+3213,/user/hadoop/cooperativa/20140101205953/part-00000:0+0,/user/hadoop/cooperativa/20140101205953/part-00001:0+0,/user/hadoop/cooperativa/20140101205953/part-00002:0+0,/user/hadoop/cooperativa/20140101205953/part-00003:0+0,/user/hadoop/cooperativa/20140101205953/part-00004:0+0,/user/hadoop/cooperativa/20140101205953/part-00005:0+2141
19/01/25 20:59:15 INFO python.PythonRunner: Times: total = 2882, boot = 317, init = 2565, finish = 0
19/01/25 20:59:15 INFO memory.MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 8.0 MB, free 3.0 GB)
19/01/25 20:59:15 INFO storage.BlockManagerInfo: Added taskresult_1 in memory on 192.168.101.140:43029 (size: 8.0 MB, free: 3.0 GB)
19/01/25 20:59:15 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 1). 8431264 bytes result sent via BlockManager)
19/01/25 20:59:15 INFO client.TransportClientFactory: Successfully created connection to /192.168.101.140:43029 after 14 ms (0 ms spent in bootstraps)
19/01/25 20:59:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 7820 ms on localhost (executor driver) (1/2)
19/01/25 20:59:15 INFO storage.BlockManagerInfo: Removed taskresult_1 on 192.168.101.140:43029 in memory (size: 8.0 MB, free: 3.0 GB)
19/01/25 20:59:20 INFO python.PythonRunner: Times: total = 11917, boot = 315, init = 11601, finish = 1
19/01/25 20:59:20 INFO memory.MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 8.0 MB, free 3.0 GB)
19/01/25 20:59:20 INFO storage.BlockManagerInfo: Added taskresult_0 in memory on 192.168.101.140:43029 (size: 8.0 MB, free: 3.0 GB)
19/01/25 20:59:20 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 0). 8431307 bytes result sent via BlockManager)
19/01/25 20:59:20 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 12794 ms on localhost (executor driver) (2/2)
19/01/25 20:59:20 INFO storage.BlockManagerInfo: Removed taskresult_0 on 192.168.101.140:43029 in memory (size: 8.0 MB, free: 3.0 GB)
19/01/25 20:59:20 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/01/25 20:59:20 INFO scheduler.DAGScheduler: ResultStage 0 (treeAggregate at IDF.scala:54) finished in 12.801 s
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Job 0 finished: treeAggregate at IDF.scala:54, took 13.300354 s
19/01/25 20:59:20 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.0 MB, free 3.0 GB)
19/01/25 20:59:20 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 43.3 KB, free 3.0 GB)
19/01/25 20:59:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.101.140:43029 (size: 43.3 KB, free: 3.0 GB)
19/01/25 20:59:20 INFO spark.SparkContext: Created broadcast 2 from broadcast at IDF.scala:177
End computing IDF
First vector:
19/01/25 20:59:20 INFO spark.SparkContext: Starting job: collect at /home/hadoop/wb-test-01/tfidf.py:42
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Got job 1 (collect at /home/hadoop/wb-test-01/tfidf.py:42) with 2 output partitions
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at /home/hadoop/wb-test-01/tfidf.py:42)
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Missing parents: List()
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at mapPartitions at PythonMLLibAPI.scala:1339), which has no missing parents
19/01/25 20:59:20 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.3 KB, free 3.0 GB)
19/01/25 20:59:20 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KB, free 3.0 GB)
19/01/25 20:59:20 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.101.140:43029 (size: 4.5 KB, free: 3.0 GB)
19/01/25 20:59:20 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
19/01/25 20:59:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at mapPartitions at PythonMLLibAPI.scala:1339) (first 15 tasks are for partitions Vector(0, 1))
19/01/25 20:59:20 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
19/01/25 20:59:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 13566 bytes)
19/01/25 20:59:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, ANY, 5518 bytes)
19/01/25 20:59:20 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
19/01/25 20:59:20 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
19/01/25 20:59:20 INFO rdd.WholeTextFileRDD: Input split: Paths:/user/hadoop/cooperativa/20140101085418/part-00005:0+2113,/user/hadoop/cooperativa/20140101090434/part-00005:0+3307,/user/hadoop/cooperativa/20140101091138/part-00005:0+1730,/user/hadoop/cooperativa/20140101100443/part-00005:0+7201,/user/hadoop/cooperativa/20140101104619/part-00005:0+3179,/user/hadoop/cooperativa/20140101110939/part-00005:0+3196,/user/hadoop/cooperativa/20140101111112/part-00005:0+2326
19/01/25 20:59:20 INFO rdd.WholeTextFileRDD: Input split: Paths:/user/hadoop/cooperativa/20140101085418/part-00000:0+0,/user/hadoop/cooperativa/20140101085418/part-00001:0+0,/user/hadoop/cooperativa/20140101085418/part-00002:0+0,/user/hadoop/cooperativa/20140101085418/part-00003:0+0,/user/hadoop/cooperativa/20140101085418/part-00004:0+0,/user/hadoop/cooperativa/20140101090434/part-00000:0+0,/user/hadoop/cooperativa/20140101090434/part-00001:0+0,/user/hadoop/cooperativa/20140101090434/part-00002:0+0,/user/hadoop/cooperativa/20140101090434/part-00003:0+0,/user/hadoop/cooperativa/20140101090434/part-00004:0+0,/user/hadoop/cooperativa/20140101091138/part-00000:0+0,/user/hadoop/cooperativa/20140101091138/part-00001:0+0,/user/hadoop/cooperativa/20140101091138/part-00002:0+0,/user/hadoop/cooperativa/20140101091138/part-00003:0+0,/user/hadoop/cooperativa/20140101091138/part-00004:0+0,/user/hadoop/cooperativa/20140101100443/part-00000:0+0,/user/hadoop/cooperativa/20140101100443/part-00001:0+0,/user/hadoop/cooperativa/20140101100443/part-00002:0+0,/user/hadoop/cooperativa/20140101100443/part-00003:0+0,/user/hadoop/cooperativa/20140101100443/part-00004:0+0,/user/hadoop/cooperativa/20140101104619/part-00000:0+0,/user/hadoop/cooperativa/20140101104619/part-00001:0+0,/user/hadoop/cooperativa/20140101104619/part-00002:0+0,/user/hadoop/cooperativa/20140101104619/part-00003:0+0,/user/hadoop/cooperativa/20140101104619/part-00004:0+0,/user/hadoop/cooperativa/20140101110939/part-00000:0+0,/user/hadoop/cooperativa/20140101110939/part-00001:0+0,/user/hadoop/cooperativa/20140101110939/part-00002:0+0,/user/hadoop/cooperativa/20140101110939/part-00003:0+0,/user/hadoop/cooperativa/20140101110939/part-00004:0+0,/user/hadoop/cooperativa/20140101111112/part-00000:0+0,/user/hadoop/cooperativa/20140101111112/part-00001:0+0,/user/hadoop/cooperativa/20140101111112/part-00002:0+0,/user/hadoop/cooperativa/20140101111112/part-00003:0+0,/user/hadoop/cooperativa/20140101111112/part-00004:0+0,/user/hadoop/cooperativa/20140101121239/part-00000:0+0,/user/hadoop/cooperativa/20140101121239/part-00001:0+0,/user/hadoop/cooperativa/20140101121239/part-00002:0+0,/user/hadoop/cooperativa/20140101121239/part-00003:0+0,/user/hadoop/cooperativa/20140101121239/part-00004:0+0,/user/hadoop/cooperativa/20140101121239/part-00005:0+2166,/user/hadoop/cooperativa/20140101130956/part-00000:0+0,/user/hadoop/cooperativa/20140101130956/part-00001:0+0,/user/hadoop/cooperativa/20140101130956/part-00002:0+0,/user/hadoop/cooperativa/20140101130956/part-00003:0+0,/user/hadoop/cooperativa/20140101130956/part-00004:0+0,/user/hadoop/cooperativa/20140101130956/part-00005:0+2058,/user/hadoop/cooperativa/20140101134236/part-00000:0+0,/user/hadoop/cooperativa/20140101134236/part-00001:0+0,/user/hadoop/cooperativa/20140101134236/part-00002:0+0,/user/hadoop/cooperativa/20140101134236/part-00003:0+0,/user/hadoop/cooperativa/20140101134236/part-00004:0+0,/user/hadoop/cooperativa/20140101134236/part-00005:0+2618,/user/hadoop/cooperativa/20140101134348/part-00000:0+0,/user/hadoop/cooperativa/20140101134348/part-00001:0+0,/user/hadoop/cooperativa/20140101134348/part-00002:0+0,/user/hadoop/cooperativa/20140101134348/part-00003:0+0,/user/hadoop/cooperativa/20140101134348/part-00004:0+0,/user/hadoop/cooperativa/20140101134348/part-00005:0+2426,/user/hadoop/cooperativa/20140101140355/part-00000:0+0,/user/hadoop/cooperativa/20140101140355/part-00001:0+0,/user/hadoop/cooperativa/20140101140355/part-00002:0+0,/user/hadoop/cooperativa/20140101140355/part-00003:0+0,/user/hadoop/cooperativa/20140101140355/part-00004:0+0,/user/hadoop/cooperativa/20140101140355/part-00005:0+1802,/user/hadoop/cooperativa/20140101141300/part-00000:0+0,/user/hadoop/cooperativa/20140101141300/part-00001:0+0,/user/hadoop/cooperativa/20140101141300/part-00002:0+0,/user/hadoop/cooperativa/20140101141300/part-00003:0+0,/user/hadoop/cooperativa/20140101141300/part-00004:0+0,/user/hadoop/cooperativa/20140101141300/part-00005:0+2806,/user/hadoop/cooperativa/20140101150649/part-00000:0+0,/user/hadoop/cooperativa/20140101150649/part-00001:0+0,/user/hadoop/cooperativa/20140101150649/part-00002:0+0,/user/hadoop/cooperativa/20140101150649/part-00003:0+0,/user/hadoop/cooperativa/20140101150649/part-00004:0+0,/user/hadoop/cooperativa/20140101150649/part-00005:0+2323,/user/hadoop/cooperativa/20140101164734/part-00000:0+0,/user/hadoop/cooperativa/20140101164734/part-00001:0+0,/user/hadoop/cooperativa/20140101164734/part-00002:0+0,/user/hadoop/cooperativa/20140101164734/part-00003:0+0,/user/hadoop/cooperativa/20140101164734/part-00004:0+0,/user/hadoop/cooperativa/20140101164734/part-00005:0+1435,/user/hadoop/cooperativa/20140101194322/part-00000:0+0,/user/hadoop/cooperativa/20140101194322/part-00001:0+0,/user/hadoop/cooperativa/20140101194322/part-00002:0+0,/user/hadoop/cooperativa/20140101194322/part-00003:0+0,/user/hadoop/cooperativa/20140101194322/part-00004:0+0,/user/hadoop/cooperativa/20140101194322/part-00005:0+3213,/user/hadoop/cooperativa/20140101205953/part-00000:0+0,/user/hadoop/cooperativa/20140101205953/part-00001:0+0,/user/hadoop/cooperativa/20140101205953/part-00002:0+0,/user/hadoop/cooperativa/20140101205953/part-00003:0+0,/user/hadoop/cooperativa/20140101205953/part-00004:0+0,/user/hadoop/cooperativa/20140101205953/part-00005:0+2141
19/01/25 20:59:20 INFO python.PythonRunner: Times: total = 72, boot = -9385, init = 9456, finish = 1
19/01/25 20:59:20 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 1806 bytes result sent to driver
19/01/25 20:59:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 90 ms on localhost (executor driver) (1/2)
19/01/25 20:59:21 INFO python.PythonRunner: Times: total = 793, boot = -346, init = 1137, finish = 2
19/01/25 20:59:21 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 5994 bytes result sent to driver
19/01/25 20:59:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 802 ms on localhost (executor driver) (2/2)
19/01/25 20:59:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/01/25 20:59:21 INFO scheduler.DAGScheduler: ResultStage 1 (collect at /home/hadoop/wb-test-01/tfidf.py:42) finished in 0.803 s
19/01/25 20:59:21 INFO scheduler.DAGScheduler: Job 1 finished: collect at /home/hadoop/wb-test-01/tfidf.py:42, took 0.808717 s
(1048576,[514297,870577],[3.9415818076696905,3.9415818076696905])
19/01/25 20:59:21 INFO spark.SparkContext: Invoking stop() from shutdown hook
19/01/25 20:59:21 INFO server.AbstractConnector: Stopped Spark@1514f16c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/01/25 20:59:21 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.101.140:4040
19/01/25 20:59:21 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/01/25 20:59:21 INFO memory.MemoryStore: MemoryStore cleared
19/01/25 20:59:21 INFO storage.BlockManager: BlockManager stopped
19/01/25 20:59:21 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
19/01/25 20:59:21 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/01/25 20:59:21 INFO spark.SparkContext: Successfully stopped SparkContext
19/01/25 20:59:21 INFO util.ShutdownHookManager: Shutdown hook called
19/01/25 20:59:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7ad65664-0d14-4839-aacc-0136eaecf10c
19/01/25 20:59:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7ad65664-0d14-4839-aacc-0136eaecf10c/pyspark-b162e699-0d44-49f9-9dc9-1eb5cc33d15f

